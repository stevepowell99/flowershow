# AIs have perspective


March 24, 2025

Do you feel uncomfortable saying "the AI misunderstood what I wanted"? Does that mean we think they are conscious? 

Silva correctly draws attention to this and has some suggestions.

I think first we have to look at uses, not words. It isn't so useful to argue about generally whether some AI is intelligent or conscious or whatever. It's interesting to look at cases where we need some of these somewhat controversial words.

Often, when talking to each other about prompt engineering, we have to say things like:

> I realised it hadn't understood what I meant by 'repeat the first part'. It was still thinking of the previous instruction, which is fair enough, I guess. So I had to rewrite that part of the prompt. 
In this kind of case, we have to use words like 'understand' or 'think' (or, more often, 'misunderstand'). We can't get round that. You can put scare-quotes round the word, or like Silva put '-simila' on the end of it, but I'm not sure what you achieve. 

The same goes for 'reasoning'. We often need to say to one another, when improving a prompt, 'ah, its reasoning was flawed here' or 'this newer model seems to do a lot more sophisticated reasoning before coming to a conclusion -- look at this part.'

I don't think I've ever, in contrast, had to use the word 'conscious' in any practical context when talking about AIs. 

You could say, 'we shouldn't use the word understand about an AI because that implies they are conscious, and how can an AI be conscious'. But I'd say that these words mean no more and no less than how we use them in this context. I have no use (at the moment -- this might change) for saying an AI is conscious, or for that matter, not conscious. 

Wittgenstein taught us to look at words in use. Beyond that is just words going on holiday: idle philosophising.